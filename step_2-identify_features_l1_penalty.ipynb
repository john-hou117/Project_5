{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 - Identify Salient Features Using $\\ell1$-penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain and Data\n",
    "\n",
    "Domain: We will utilize our machine learning pipeline to programatically select relevant features from a vast number of features.\n",
    "\n",
    "Data: Our dataset is the same from Step 1 (Benchmarking), the MADELON dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "The task at hand is to identify only the relevant, informative features of our dataset. We have a total of 500 features. Of the 500 features, 5 features are actually informative, and 15 are linear combinations of the 5 informative features, which gives us a total of 20 salient features. Thus, this means that the remaining features (480) are essentially noise (i.e. distractors), and should be eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Statement\n",
    "\n",
    "To reduce the noise, and thus identify relevant features, we will use our constructed pipeline, using the Logistic Regression model with a L1 penalty (Lasso regularization). The Lasso will drive the coefficients of the noise features to 0, thus eliminating these distractors.\n",
    "\n",
    "As in Step 1 - Benchmarking, our pipeline will run through these steps. The only difference will be that in our final step, the general_model, our Logistic Regression will use penalty='l1', the Lasso regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/identify_features.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import our wrapper functions from the project_5.py in our lib\n",
    "from lib.project_5 import load_data_from_database, add_to_process_list, make_data_dict, validate_dictionary, general_model, general_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load our data, from the database, into a DataFrame\n",
    "madelon_df = load_data_from_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 501)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure our data was loaded correctly. Our DataFrame should have 2000 rows and 501 columns\n",
    "madelon_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a data dictionary from our DataFrame\n",
    "data_dictionary = make_data_dict(madelon_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 0.79002585,  1.29603583,  1.67665391, ...,  0.53934519,\n",
       "         -0.42435928, -0.26481217],\n",
       "        [ 1.41226843,  0.47200491,  0.07523707, ...,  0.75829452,\n",
       "         -0.50332383,  0.3560579 ],\n",
       "        [-0.45445931,  0.07647007, -0.4155197 , ...,  0.46636208,\n",
       "         -0.58228839,  0.62768856],\n",
       "        ..., \n",
       "        [-0.29889867,  2.02118304,  0.17855429, ..., -0.48241836,\n",
       "         -0.34539472,  1.13214549],\n",
       "        [-0.14333802, -1.07717322, -0.23471457, ...,  1.26917629,\n",
       "          0.44425082,  1.48138491],\n",
       "        [-1.07670189, -0.35202601, -1.34537464, ...,  0.46636208,\n",
       "         -2.29318707,  0.31725352]]),\n",
       " 'X_train': array([[-0.29889867, -0.02241364, -0.82878856, ..., -1.06628325,\n",
       "          0.36528627,  0.74410169],\n",
       "        [-0.92114125, -2.56042887,  0.97926271, ..., -0.19048592,\n",
       "         -0.16114409, -1.23492166],\n",
       "        [-2.78786899,  1.29603583, -0.05390945, ..., -0.0445197 ,\n",
       "          0.23367868, -0.65285597],\n",
       "        ..., \n",
       "        [ 0.79002585, -0.71459961, -0.07973875, ...,  0.75829452,\n",
       "          0.10207109,  0.27844914],\n",
       "        [-0.7655806 ,  0.63681109,  0.9534334 , ..., -0.11750281,\n",
       "         -0.16114409, -0.42002969],\n",
       "        [ 1.25670778,  0.20831502,  1.10840922, ...,  0.97724385,\n",
       "          0.47057234, -1.04089976]]),\n",
       " 'processes': [StandardScaler(copy=True, with_mean=True, with_std=True)],\n",
       " 'y_test': index\n",
       " 1542   -1\n",
       " 394     1\n",
       " 158    -1\n",
       " 731    -1\n",
       " 1752    1\n",
       " 1939   -1\n",
       " 1280    1\n",
       " 949     1\n",
       " 1713   -1\n",
       " 638    -1\n",
       " 431    -1\n",
       " 1539    1\n",
       " 375    -1\n",
       " 1498   -1\n",
       " 1722    1\n",
       " 1578    1\n",
       " 551     1\n",
       " 673     1\n",
       " 1555    1\n",
       " 1175   -1\n",
       " 1119    1\n",
       " 1162   -1\n",
       " 216     1\n",
       " 682     1\n",
       " 543    -1\n",
       " 1922    1\n",
       " 1133    1\n",
       " 552    -1\n",
       " 460     1\n",
       " 157    -1\n",
       "        ..\n",
       " 1556   -1\n",
       " 1029    1\n",
       " 490    -1\n",
       " 235    -1\n",
       " 1859   -1\n",
       " 1683   -1\n",
       " 65     -1\n",
       " 1225    1\n",
       " 33      1\n",
       " 536    -1\n",
       " 1579    1\n",
       " 1145    1\n",
       " 637    -1\n",
       " 379     1\n",
       " 1987    1\n",
       " 1245   -1\n",
       " 1665    1\n",
       " 773     1\n",
       " 776    -1\n",
       " 236     1\n",
       " 1968   -1\n",
       " 1232    1\n",
       " 1927   -1\n",
       " 1770    1\n",
       " 1353   -1\n",
       " 1985    1\n",
       " 1080   -1\n",
       " 1387    1\n",
       " 1845    1\n",
       " 93      1\n",
       " Name: label, dtype: int64,\n",
       " 'y_train': index\n",
       " 1719    1\n",
       " 986     1\n",
       " 1156    1\n",
       " 624    -1\n",
       " 1619    1\n",
       " 1706    1\n",
       " 784    -1\n",
       " 705    -1\n",
       " 1855   -1\n",
       " 360     1\n",
       " 1259   -1\n",
       " 1442    1\n",
       " 230     1\n",
       " 1177    1\n",
       " 905    -1\n",
       " 600     1\n",
       " 967    -1\n",
       " 635     1\n",
       " 449     1\n",
       " 254    -1\n",
       " 737    -1\n",
       " 1062   -1\n",
       " 200    -1\n",
       " 871     1\n",
       " 829    -1\n",
       " 1746    1\n",
       " 1371   -1\n",
       " 1839    1\n",
       " 562    -1\n",
       " 1966   -1\n",
       "        ..\n",
       " 1749    1\n",
       " 1765   -1\n",
       " 886     1\n",
       " 1630    1\n",
       " 1648    1\n",
       " 1974   -1\n",
       " 149    -1\n",
       " 1777    1\n",
       " 1745    1\n",
       " 1944    1\n",
       " 1238   -1\n",
       " 653     1\n",
       " 1766    1\n",
       " 1952    1\n",
       " 1270   -1\n",
       " 412     1\n",
       " 184    -1\n",
       " 486    -1\n",
       " 434     1\n",
       " 1096   -1\n",
       " 1669   -1\n",
       " 1976    1\n",
       " 1986   -1\n",
       " 398     1\n",
       " 1546    1\n",
       " 1590   -1\n",
       " 685    -1\n",
       " 622    -1\n",
       " 856    -1\n",
       " 206     1\n",
       " Name: label, dtype: int64}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform our data using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled = general_transformer(StandardScaler(), data_dictionary)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 0.79002585,  1.29603583,  1.67665391, ...,  0.53934519,\n",
       "         -0.42435928, -0.26481217],\n",
       "        [ 1.41226843,  0.47200491,  0.07523707, ...,  0.75829452,\n",
       "         -0.50332383,  0.3560579 ],\n",
       "        [-0.45445931,  0.07647007, -0.4155197 , ...,  0.46636208,\n",
       "         -0.58228839,  0.62768856],\n",
       "        ..., \n",
       "        [-0.29889867,  2.02118304,  0.17855429, ..., -0.48241836,\n",
       "         -0.34539472,  1.13214549],\n",
       "        [-0.14333802, -1.07717322, -0.23471457, ...,  1.26917629,\n",
       "          0.44425082,  1.48138491],\n",
       "        [-1.07670189, -0.35202601, -1.34537464, ...,  0.46636208,\n",
       "         -2.29318707,  0.31725352]]),\n",
       " 'X_train': array([[-0.29889867, -0.02241364, -0.82878856, ..., -1.06628325,\n",
       "          0.36528627,  0.74410169],\n",
       "        [-0.92114125, -2.56042887,  0.97926271, ..., -0.19048592,\n",
       "         -0.16114409, -1.23492166],\n",
       "        [-2.78786899,  1.29603583, -0.05390945, ..., -0.0445197 ,\n",
       "          0.23367868, -0.65285597],\n",
       "        ..., \n",
       "        [ 0.79002585, -0.71459961, -0.07973875, ...,  0.75829452,\n",
       "          0.10207109,  0.27844914],\n",
       "        [-0.7655806 ,  0.63681109,  0.9534334 , ..., -0.11750281,\n",
       "         -0.16114409, -0.42002969],\n",
       "        [ 1.25670778,  0.20831502,  1.10840922, ...,  0.97724385,\n",
       "          0.47057234, -1.04089976]]),\n",
       " 'coef_': array([[ -8.88543089e-02,   1.34352643e-01,   7.68766648e-02,\n",
       "          -1.42286490e-01,   1.86519363e-01,  -1.33599690e-01,\n",
       "           1.15796647e-01,   8.20975352e-04,   5.52773088e-02,\n",
       "           4.15406930e-02,  -2.11837378e-01,   6.77317464e-02,\n",
       "           1.19494867e-01,   0.00000000e+00,  -1.16854220e-01,\n",
       "           8.81405022e-02,  -1.27821149e-02,   1.44469150e-02,\n",
       "           1.41523065e-01,  -9.90845476e-02,  -1.44706106e-01,\n",
       "           1.09825231e-01,  -5.16538093e-03,  -1.01713546e-01,\n",
       "           1.05529876e-01,   6.93986163e-02,   1.37427788e-01,\n",
       "           9.83669105e-02,  -1.11495013e+00,  -6.18357928e-02,\n",
       "           1.26201395e-02,  -4.70881396e-02,   1.14561565e-02,\n",
       "           4.78942083e-03,  -1.52916824e-01,  -5.68111885e-02,\n",
       "          -8.97593618e-02,  -2.49126760e-03,  -9.24068573e-02,\n",
       "           3.94093176e-03,   9.13070163e-02,   4.96227829e-02,\n",
       "          -1.18995971e-01,   5.33555839e-03,   1.60099921e-01,\n",
       "           4.44874178e-02,   2.43216443e-01,   9.60350030e-02,\n",
       "           3.61084106e-01,  -2.63939308e-01,   1.32302636e-01,\n",
       "          -1.25101231e-01,   1.10895963e-01,   6.79817585e-02,\n",
       "           4.03671461e-02,  -8.25753397e-02,   1.54640688e-01,\n",
       "          -7.23314764e-02,  -5.90779695e-04,  -8.09218228e-03,\n",
       "          -9.53170273e-02,   1.68849281e-01,   8.54460012e-02,\n",
       "          -1.67985487e-01,   0.00000000e+00,  -1.54313075e-01,\n",
       "          -9.36219708e-02,   6.20478049e-02,   0.00000000e+00,\n",
       "          -6.24169624e-02,  -6.67016437e-02,  -9.52625999e-03,\n",
       "           0.00000000e+00,   2.60393997e-01,   1.14200173e-01,\n",
       "           1.27290941e-01,   0.00000000e+00,   1.66133917e-01,\n",
       "          -1.14099774e-01,   1.05626818e-01,   0.00000000e+00,\n",
       "          -1.32865588e-01,  -5.97392154e-02,   1.22722230e-01,\n",
       "          -1.59283603e-01,  -1.15775230e-01,   1.65171369e-02,\n",
       "          -1.57199040e-01,  -8.61783479e-02,  -5.46613918e-02,\n",
       "           6.88691651e-02,  -8.03097756e-02,  -8.04607646e-02,\n",
       "           1.08700824e-02,  -4.60769462e-02,   1.92766036e-02,\n",
       "           9.03577258e-02,   8.76331496e-02,   6.85929657e-02,\n",
       "          -7.74553824e-02,   7.57489472e-02,   5.11406492e-02,\n",
       "           1.43703150e-01,  -1.89546030e-01,   0.00000000e+00,\n",
       "           4.18920418e-02,   0.00000000e+00,  -7.91171750e-02,\n",
       "          -3.12222316e-02,  -3.56622535e-02,   8.05762846e-02,\n",
       "           7.56025874e-02,  -8.62898981e-02,  -2.83840265e-02,\n",
       "           1.70963056e-01,  -9.45731494e-02,   2.20596191e-03,\n",
       "           4.98042624e-02,   0.00000000e+00,   9.75738128e-02,\n",
       "           1.11539078e-01,   1.54917826e-03,   1.93740663e-02,\n",
       "          -1.95464618e-02,   4.97367740e-02,  -1.20950323e-01,\n",
       "          -7.28729514e-02,  -1.64314255e-01,  -2.13870008e-03,\n",
       "          -2.30637198e-02,  -2.90909405e-01,  -9.80608844e-03,\n",
       "          -7.40112950e-02,  -8.74715416e-03,   1.27925962e-01,\n",
       "           3.66877307e-02,   1.14822396e-01,   2.14853443e-01,\n",
       "           3.55651224e-02,  -2.38557064e-01,   1.01265617e-01,\n",
       "           9.57899199e-02,   3.59385731e-02,  -3.89914762e-02,\n",
       "           9.97402903e-02,  -6.40085961e-02,  -9.25726689e-02,\n",
       "          -1.13396196e-01,  -8.07824055e-02,  -2.14492159e-02,\n",
       "          -7.24873856e-02,   5.05791852e-02,  -1.71145568e-01,\n",
       "          -9.91833700e-03,  -8.07466051e-02,  -1.23149176e-01,\n",
       "          -8.26712243e-02,  -8.27309623e-02,   5.52032043e-02,\n",
       "          -7.66167505e-02,   0.00000000e+00,  -1.52951253e-01,\n",
       "          -8.79734901e-02,   0.00000000e+00,   2.04427923e-01,\n",
       "           1.27498158e-01,  -1.62545195e-01,  -5.05541893e-02,\n",
       "          -1.99000224e-01,  -6.32641407e-04,   0.00000000e+00,\n",
       "           9.26830464e-02,   0.00000000e+00,   4.94546181e-02,\n",
       "          -3.63781601e-03,  -1.87125079e-01,  -3.21769673e-02,\n",
       "          -1.02418563e-01,   4.55119793e-02,  -1.04115731e-01,\n",
       "           0.00000000e+00,   5.29876207e-02,   1.07896048e-01,\n",
       "           8.59903430e-03,   2.38357204e-02,   7.73874149e-03,\n",
       "           1.08090413e-01,  -8.56760486e-05,  -1.18291963e-02,\n",
       "           9.23188534e-02,  -9.03562039e-03,   7.13103003e-02,\n",
       "           5.45170540e-03,   3.25650547e-02,   0.00000000e+00,\n",
       "           4.27435583e-02,   1.07980897e-01,   4.93558905e-02,\n",
       "           2.73541063e-02,  -5.51692916e-02,  -2.18384401e-02,\n",
       "           1.92448583e-02,  -1.00958869e-01,   3.38149082e-02,\n",
       "           1.31793812e-01,  -6.43358229e-02,   8.08302115e-03,\n",
       "           1.19771906e-01,  -5.56955476e-02,  -7.38475584e-02,\n",
       "           0.00000000e+00,   1.61119090e-01,   3.96385824e-02,\n",
       "          -1.40621387e-01,   1.50632946e-01,   4.30628085e-02,\n",
       "          -1.71246190e-01,   5.24995783e-02,   9.57763120e-03,\n",
       "          -2.82244616e-02,  -6.38132078e-02,  -1.88942698e-01,\n",
       "          -3.76342382e-02,  -9.02801808e-03,  -5.78535543e-02,\n",
       "           4.85753766e-02,  -1.93229927e-01,  -8.95930949e-03,\n",
       "          -6.22175819e-02,   9.91824164e-02,   1.28843152e-01,\n",
       "           2.01246041e-02,   0.00000000e+00,   6.51362836e-02,\n",
       "           2.45523915e-02,  -4.46113632e-02,  -1.96085345e-01,\n",
       "          -1.23566521e-01,  -4.73565050e-02,  -8.89177085e-02,\n",
       "           1.57037924e-01,   5.46662949e-01,   7.08117291e-02,\n",
       "          -8.85443309e-02,   5.27188367e-02,   1.42347907e-01,\n",
       "          -3.57858448e-02,   6.63062107e-02,   1.34820298e-01,\n",
       "           5.14014237e-02,  -3.47714312e-02,   2.04897853e-03,\n",
       "           1.29745996e-01,  -8.03258802e-02,   3.30554843e-02,\n",
       "           1.03858932e-01,  -1.64180390e-01,  -1.38359551e-01,\n",
       "          -2.34545920e-01,  -5.91986062e-02,   1.02957464e-02,\n",
       "          -1.81229176e-01,   4.32823689e-02,  -2.62552712e-02,\n",
       "           1.69647701e-02,   3.26170177e-02,   1.13945730e-01,\n",
       "          -2.64348486e-02,   2.81709032e-02,   0.00000000e+00,\n",
       "           1.18048017e-01,   0.00000000e+00,  -1.23292149e-01,\n",
       "           1.11801848e-01,  -1.83517431e-02,   4.42847157e-02,\n",
       "           3.40109460e-02,  -7.88679118e-02,  -1.69613402e-01,\n",
       "          -1.01887301e-02,   4.72713173e-02,  -3.48196615e-01,\n",
       "           1.34063688e-01,  -3.23930739e-02,   0.00000000e+00,\n",
       "          -1.19882142e-01,  -2.68521410e-01,  -1.24583443e-01,\n",
       "           8.97896708e-02,   6.13242721e-02,   1.74003650e-01,\n",
       "          -1.32827585e-01,  -8.80446228e-02,  -1.43663833e-01,\n",
       "          -2.40394690e-02,  -9.22888036e-02,  -1.09169863e-01,\n",
       "          -1.23729704e-01,  -3.74653479e-02,   1.15205352e-01,\n",
       "           4.11143659e-02,   7.05630104e-02,   1.58665400e-01,\n",
       "          -9.39432794e-02,  -1.38583024e-02,   5.07703881e-02,\n",
       "          -1.46109257e-01,   1.83787436e-01,   3.37721015e-02,\n",
       "           1.75685851e-01,  -1.15775594e-01,   9.97861275e-02,\n",
       "           4.00012907e-02,  -3.88299395e-02,   3.77208819e-02,\n",
       "           8.70275781e-04,  -1.01725995e-01,   1.64271586e-01,\n",
       "          -1.10639449e-01,  -4.26952748e-03,   2.24775550e-02,\n",
       "           3.14627900e-01,  -9.57321286e-02,  -7.15858708e-02,\n",
       "           5.53582463e-02,  -4.68138988e-03,   4.47462133e-04,\n",
       "          -1.08045342e-01,   5.09418187e-02,   9.06861613e-02,\n",
       "           1.27974815e-01,   5.33878551e-02,  -1.32961598e-01,\n",
       "           2.01577603e-01,  -1.36540292e-01,  -1.02129369e-01,\n",
       "          -1.31912627e-01,  -5.67538051e-02,   0.00000000e+00,\n",
       "           9.32883505e-02,   1.79572392e-01,   5.19985531e-02,\n",
       "           9.01600295e-02,  -1.43567882e-01,   7.04300802e-02,\n",
       "           0.00000000e+00,   9.09408212e-03,  -9.51378617e-02,\n",
       "           1.21795698e-01,   1.57477903e-02,   4.15311254e-02,\n",
       "           3.55855468e-02,   1.22340012e-01,   1.05342698e-01,\n",
       "          -5.12832392e-02,  -4.37096933e-02,  -9.11849449e-02,\n",
       "          -3.63540882e-02,   7.69874802e-02,  -6.80245331e-02,\n",
       "           8.82261099e-03,   9.82551663e-02,  -1.59897177e-01,\n",
       "           8.16610521e-02,   8.40264877e-02,  -1.39701103e-03,\n",
       "           0.00000000e+00,  -1.03780773e-02,   6.83383816e-02,\n",
       "          -6.82511015e-02,   4.06648117e-02,  -3.40317787e-02,\n",
       "           8.12750074e-02,   0.00000000e+00,   4.74280205e-02,\n",
       "          -4.44391502e-02,   1.89976434e-01,  -2.22319331e-01,\n",
       "          -1.05532103e-01,   2.06674712e-02,  -5.94785887e-02,\n",
       "          -3.43499645e-02,   9.75195364e-02,  -1.86251270e-02,\n",
       "           1.16443072e-01,   5.03278889e-02,   0.00000000e+00,\n",
       "           5.91278953e-03,  -1.10558697e-01,  -4.96670422e-02,\n",
       "           0.00000000e+00,   0.00000000e+00,  -6.64811902e-02,\n",
       "           1.85095294e-02,  -6.72819019e-02,  -1.04969798e-01,\n",
       "          -9.76600613e-02,  -4.72300000e-02,   6.25955248e-02,\n",
       "           1.64009363e-01,  -2.42206648e-02,   3.55504057e-02,\n",
       "          -7.95718216e-02,   1.98043937e-01,  -1.15708805e-01,\n",
       "           7.67021940e-02,   0.00000000e+00,  -4.81644605e-02,\n",
       "           9.49501151e-02,  -2.53344762e-02,   9.55612098e-03,\n",
       "          -1.26339440e-01,   2.27154433e-01,  -2.38367359e-01,\n",
       "          -1.51446241e-01,  -4.86171159e-02,  -1.06906654e-01,\n",
       "          -2.48528757e-01,  -1.52398900e-02,   1.44951508e-01,\n",
       "          -2.53257425e-02,  -1.36686693e-01,  -8.37551944e-02,\n",
       "          -9.36014167e-02,   2.33323524e-01,  -3.04573057e-01,\n",
       "          -7.73790365e-02,   8.07596843e-02,  -1.02433273e-02,\n",
       "          -1.19482412e-02,  -2.46917504e-01,   1.35003216e-01,\n",
       "           1.18042626e-01,   3.64947188e-01,   9.89183031e-02,\n",
       "           1.01841532e-01,   7.66281317e-02,  -1.28950347e-01,\n",
       "          -8.75897944e-02,   8.68505551e-02,  -5.97539579e-04,\n",
       "          -1.37461347e-01,  -8.53194061e-02,   1.19165841e-02,\n",
       "          -2.87410706e-04,   2.10738419e-01,   0.00000000e+00,\n",
       "          -1.57926212e-01,  -1.09645115e-01,  -2.33997594e-01,\n",
       "          -4.49273257e-02,   1.19070709e+00,   2.12684990e-01,\n",
       "           0.00000000e+00,   1.52080965e-01,   0.00000000e+00,\n",
       "           8.84574487e-02,   9.22268540e-02,   2.31731322e-01,\n",
       "          -2.06504317e-03,   5.66517976e-02,   1.22844090e-01,\n",
       "          -7.48943293e-03,  -2.74899032e-02,  -3.04751078e-02,\n",
       "           3.41274462e-02,   3.27378547e-02,  -6.52466149e-02,\n",
       "          -1.09211153e-02,  -3.79420252e-03,   5.79754011e-02,\n",
       "           2.11663989e-01,   0.00000000e+00,   2.08264428e-02,\n",
       "          -5.74400588e-03,   0.00000000e+00,   0.00000000e+00,\n",
       "           5.26487128e-02,  -6.84846279e-03,  -9.54797515e-02,\n",
       "          -1.10670456e-01,  -2.16323605e-01,   1.68362472e-02,\n",
       "           6.07362817e-03,   0.00000000e+00,  -1.38375776e-01,\n",
       "           5.16583937e-02,   1.25564962e-01,  -7.42552937e-02,\n",
       "           8.76471605e-02,   1.19719881e-01,   0.00000000e+00,\n",
       "           6.93057380e-03,   0.00000000e+00,   9.98420105e-02,\n",
       "          -2.13571262e-01,  -2.05107627e-01,   1.36723963e-01,\n",
       "           5.09435116e-02,   1.81765513e-01]]),\n",
       " 'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False)],\n",
       " 'sal_features': array([[-0.29889867, -0.02241364, -0.82878856, ..., -1.06628325,\n",
       "          0.36528627,  0.74410169],\n",
       "        [-0.92114125, -2.56042887,  0.97926271, ..., -0.19048592,\n",
       "         -0.16114409, -1.23492166],\n",
       "        [-2.78786899,  1.29603583, -0.05390945, ..., -0.0445197 ,\n",
       "          0.23367868, -0.65285597],\n",
       "        ..., \n",
       "        [ 0.79002585, -0.71459961, -0.07973875, ...,  0.75829452,\n",
       "          0.10207109,  0.27844914],\n",
       "        [-0.7655806 ,  0.63681109,  0.9534334 , ..., -0.11750281,\n",
       "         -0.16114409, -0.42002969],\n",
       "        [ 1.25670778,  0.20831502,  1.10840922, ...,  0.97724385,\n",
       "          0.47057234, -1.04089976]]),\n",
       " 'test_score': 0.49833333333333335,\n",
       " 'train_score': 0.79571428571428571,\n",
       " 'y_test': index\n",
       " 1542   -1\n",
       " 394     1\n",
       " 158    -1\n",
       " 731    -1\n",
       " 1752    1\n",
       " 1939   -1\n",
       " 1280    1\n",
       " 949     1\n",
       " 1713   -1\n",
       " 638    -1\n",
       " 431    -1\n",
       " 1539    1\n",
       " 375    -1\n",
       " 1498   -1\n",
       " 1722    1\n",
       " 1578    1\n",
       " 551     1\n",
       " 673     1\n",
       " 1555    1\n",
       " 1175   -1\n",
       " 1119    1\n",
       " 1162   -1\n",
       " 216     1\n",
       " 682     1\n",
       " 543    -1\n",
       " 1922    1\n",
       " 1133    1\n",
       " 552    -1\n",
       " 460     1\n",
       " 157    -1\n",
       "        ..\n",
       " 1556   -1\n",
       " 1029    1\n",
       " 490    -1\n",
       " 235    -1\n",
       " 1859   -1\n",
       " 1683   -1\n",
       " 65     -1\n",
       " 1225    1\n",
       " 33      1\n",
       " 536    -1\n",
       " 1579    1\n",
       " 1145    1\n",
       " 637    -1\n",
       " 379     1\n",
       " 1987    1\n",
       " 1245   -1\n",
       " 1665    1\n",
       " 773     1\n",
       " 776    -1\n",
       " 236     1\n",
       " 1968   -1\n",
       " 1232    1\n",
       " 1927   -1\n",
       " 1770    1\n",
       " 1353   -1\n",
       " 1985    1\n",
       " 1080   -1\n",
       " 1387    1\n",
       " 1845    1\n",
       " 93      1\n",
       " Name: label, dtype: int64,\n",
       " 'y_train': index\n",
       " 1719    1\n",
       " 986     1\n",
       " 1156    1\n",
       " 624    -1\n",
       " 1619    1\n",
       " 1706    1\n",
       " 784    -1\n",
       " 705    -1\n",
       " 1855   -1\n",
       " 360     1\n",
       " 1259   -1\n",
       " 1442    1\n",
       " 230     1\n",
       " 1177    1\n",
       " 905    -1\n",
       " 600     1\n",
       " 967    -1\n",
       " 635     1\n",
       " 449     1\n",
       " 254    -1\n",
       " 737    -1\n",
       " 1062   -1\n",
       " 200    -1\n",
       " 871     1\n",
       " 829    -1\n",
       " 1746    1\n",
       " 1371   -1\n",
       " 1839    1\n",
       " 562    -1\n",
       " 1966   -1\n",
       "        ..\n",
       " 1749    1\n",
       " 1765   -1\n",
       " 886     1\n",
       " 1630    1\n",
       " 1648    1\n",
       " 1974   -1\n",
       " 149    -1\n",
       " 1777    1\n",
       " 1745    1\n",
       " 1944    1\n",
       " 1238   -1\n",
       " 653     1\n",
       " 1766    1\n",
       " 1952    1\n",
       " 1270   -1\n",
       " 412     1\n",
       " 184    -1\n",
       " 486    -1\n",
       " 434     1\n",
       " 1096   -1\n",
       " 1669   -1\n",
       " 1976    1\n",
       " 1986   -1\n",
       " 398     1\n",
       " 1546    1\n",
       " 1590   -1\n",
       " 685    -1\n",
       " 622    -1\n",
       " 856    -1\n",
       " 206     1\n",
       " Name: label, dtype: int64}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Logistic Regression, but this time with Lasso regularization (denoted by the passed\n",
    "# parameter penalty='l1')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scored = general_model(LogisticRegression(penalty='l1'), scaled)\n",
    "scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many salient features are remaining after using our Lasso penalty\n",
    "scored['sal_features'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "# Verify the above number (463) is actually the number of salient features remaining. We\n",
    "# will do this by checking the value of each coefficient. If the value is 0, then that feature\n",
    "# was eliminated. This means that if we subtract the total number of features (500) from the\n",
    "# number of salient features remaining, we will be left with the number of features that were\n",
    "# eliminated. Thus, the number we are looking for is 500 - 463 = 37. \n",
    "counter = 0\n",
    "for coef in scored['coef_'].flat:\n",
    "    if coef == 0.0:\n",
    "        counter += 1\n",
    "print counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric\n",
    "\n",
    "We will use the number of salient features remaining as our metric. This means that the coefficient of that salient feature will not be zero, as our Lasso penalty should drive the coefficients of irrelevant features to zero.\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "Our scrum overlord, Joshua Cook, has instructed us that if we are left with 30 features or less, we should consider this a win.\n",
    "\n",
    "### Results\n",
    "\n",
    "As you can see, we fell very short of our benchmark number of 30 salient features or less. After running our Logistic Regression with Lasso, we were still left with 463 supposedly salient features. Of course, we know there aren't 463 relevant features, since the MADELON dataset actually contains only 20 relevant features. Also, though we weren't using test score accuracy as our benchmark, it is important to note that running Lasso (as opposed to using the default 'l2' Ridge from our Step-1 Benchmark result) actually *hurt* our accuracy, as we scored a less than stellar 49%.\n",
    "\n",
    "Perhaps running our Logistic Regression again with a different C (the default is 1.0) will allow for better feature selection, along with an improved test score. We will explore other options in our Step 3 Jupyter Notebook, when we run our model through a GridSearch to exhaustively search for optimal parameters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
